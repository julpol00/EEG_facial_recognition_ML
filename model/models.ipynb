{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import mne\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Helper Functions",
   "id": "fe6593ecf0d65a1f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def split_train_test_path_list(data_path, file_name_template, train_ratio):\n",
    "    file_list = sorted(glob.glob(os.path.join(data_path, file_name_template)))\n",
    "    np.random.shuffle(file_list)\n",
    "    split_id = int(len(file_list) * train_ratio)\n",
    "\n",
    "    train_list = file_list[:split_id]\n",
    "    test_list = file_list[split_id:]\n",
    "\n",
    "    return train_list, test_list\n",
    "\n",
    "def read_eeg_epochs(train_list, test_list):\n",
    "    epochs_train_list = []\n",
    "    epochs_test_list = []\n",
    "\n",
    "    for file_path in train_list:\n",
    "        with mne.utils.use_log_level(\"ERROR\"):\n",
    "            epoch_train = mne.read_epochs(file_path, preload=True)\n",
    "            epochs_train_list.append(epoch_train)\n",
    "\n",
    "    for file_path in test_list:\n",
    "        with mne.utils.use_log_level(\"ERROR\"):\n",
    "            epoch_test = mne.read_epochs(file_path, preload=True)\n",
    "            epochs_test_list.append(epoch_test)\n",
    "\n",
    "    epochs_train = mne.concatenate_epochs(epochs_train_list)\n",
    "    epochs_test = mne.concatenate_epochs(epochs_test_list)\n",
    "\n",
    "    return epochs_train, epochs_test\n",
    "\n",
    "\n",
    "def get_window_idx(center, width, times_arr):\n",
    "    return np.where((times_arr >= center - width/2) & (times_arr <= center + width/2))[0]\n",
    "\n",
    "def zero_crossings(sig):\n",
    "    return np.where(np.diff(np.sign(sig)))[0].size\n",
    "\n",
    "def extract_peak_features(X, times, window_size=0.04, baseline_correction = False):\n",
    "\n",
    "    n_epochs, n_channels, n_times = X.shape\n",
    "    features = []\n",
    "\n",
    "    peak_window_N170 =(0.12, 0.22)\n",
    "    peak_window_P100=(0.05, 0.15)\n",
    "    peak_window_P300 = (0.2, 0.3)\n",
    "\n",
    "    win_mask_N170 = (times >= peak_window_N170[0]) & (times <= peak_window_N170[1])\n",
    "    win_mask_P100 = (times >= peak_window_P100[0]) & (times <= peak_window_P100[1])\n",
    "    win_mask_P300 = (times >= peak_window_P300[0]) & (times <= peak_window_P300[1])\n",
    "\n",
    "    for ep in range(n_epochs):\n",
    "        feats_ep = []\n",
    "        for ch in range(n_channels):\n",
    "            signal = X[ep, ch, :]\n",
    "\n",
    "\n",
    "            sig_win_P100 = signal[win_mask_P100]\n",
    "            times_win_P100 = times[win_mask_P100]\n",
    "\n",
    "            peaks, _ = find_peaks(sig_win_P100, distance=100)\n",
    "            if len(peaks) == 0:\n",
    "                idx1 = np.argmax(sig_win_P100)\n",
    "            else:\n",
    "                idx1 = peaks[0]\n",
    "            time1 = times_win_P100[idx1]\n",
    "            amp1 = float(sig_win_P100[idx1])\n",
    "            if baseline_correction:\n",
    "                amp1 -= float(sig_win_P100[0])\n",
    "\n",
    "\n",
    "            sig_win_N170 = signal[win_mask_N170]\n",
    "            times_win_N170 = times[win_mask_N170]\n",
    "\n",
    "            peaks_min, _ = find_peaks(-sig_win_N170, distance=100)\n",
    "            if len(peaks_min) == 0:\n",
    "                idx2 = np.argmin(sig_win_N170)\n",
    "            else:\n",
    "                idx2 = peaks_min[0]\n",
    "            time2 = times_win_N170[idx2]\n",
    "            amp2 = float(sig_win_N170[idx2])\n",
    "            if baseline_correction:\n",
    "                amp2 -= float(sig_win_N170[0])\n",
    "\n",
    "\n",
    "            sig_win_P300 = signal[win_mask_P300]\n",
    "            times_win_P300 = times[win_mask_P300]\n",
    "\n",
    "            peaks, _ = find_peaks(sig_win_P300, distance=100)\n",
    "            if len(peaks) == 0:\n",
    "                idx3 = np.argmax(sig_win_P300)\n",
    "            else:\n",
    "                idx3 = peaks[0]\n",
    "            time3 = times_win_P300[idx3]\n",
    "            amp3 = float(sig_win_P300[idx3])\n",
    "            if baseline_correction:\n",
    "                amp3 -= float(sig_win_P300[0])\n",
    "\n",
    "            win1 = get_window_idx(time1, window_size, times)\n",
    "            win2 = get_window_idx(time2, window_size, times)\n",
    "            win3 = get_window_idx(time3, window_size, times)\n",
    "\n",
    "            feats_ep.extend([time1, time2, time3, amp1, amp2, amp3])\n",
    "\n",
    "            for win in [win1, win2, win3]:\n",
    "                if len(win) > 0:\n",
    "                    sig_win = signal[win]\n",
    "                    t_win = times[win]\n",
    "\n",
    "                    # 7. Zero-crossing rate\n",
    "                    feat_zc = zero_crossings(sig_win)\n",
    "                    # 8. Peak-to-peak amplitude\n",
    "                    feat_ptp = np.ptp(sig_win)\n",
    "\n",
    "                    if baseline_correction:\n",
    "                        sig_win = sig_win - sig_win[0]\n",
    "\n",
    "                    # Feature calculations:\n",
    "                    # 1. RMS\n",
    "                    feat_rms = np.sqrt(np.mean(sig_win ** 2))\n",
    "                    # 2. Variance\n",
    "                    feat_var = np.var(sig_win)\n",
    "                    # 3. Std\n",
    "                    feat_std = np.std(sig_win)\n",
    "                    # 4. Skewness\n",
    "                    feat_skew = skew(sig_win)\n",
    "                    # 5. Kurtosis\n",
    "                    feat_kurt = kurtosis(sig_win)\n",
    "                    # 6. Area under the curve\n",
    "                    feat_auc = np.sum(sig_win)\n",
    "                    # 9. Slope\n",
    "                    # Fit line: polyfit returns [slope, intercept]\n",
    "                    slope = np.polyfit(t_win, sig_win, 1)[0] if len(sig_win) > 1 else np.nan\n",
    "                    # 10. Mean\n",
    "                    feat_mean = np.mean(sig_win)\n",
    "                    # 11. Min\n",
    "                    feat_min = np.min(sig_win)\n",
    "                    # 12. Max\n",
    "                    feat_max = np.max(sig_win)\n",
    "                    # 13. Median\n",
    "                    feat_median = np.median(sig_win)\n",
    "\n",
    "                    feats_ep.extend([\n",
    "                        feat_rms, feat_var, feat_std, feat_skew, feat_kurt, feat_auc, feat_zc, feat_ptp, slope, feat_mean, feat_min, feat_max, feat_median\n",
    "                    ])\n",
    "                else:\n",
    "                    feats_ep.extend([np.nan]*14)\n",
    "\n",
    "        features.append(feats_ep)\n",
    "    return np.array(features)  # shape: (n_epochs, n_channels*3*14)\n",
    "\n",
    "\n",
    "def get_X_and_Y_from_epochs(train_list, test_list, events, picks=None, t_min = -0.2, t_max = 0.5):\n",
    "\n",
    "    epochs_train, epochs_test = read_eeg_epochs(train_list, test_list)\n",
    "\n",
    "    #####---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    epochs_train_list_event1 = epochs_train[events[0]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    epochs_train_list_event2 = epochs_train[events[1]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    X_train = np.concatenate((epochs_train_list_event1, epochs_train_list_event2), axis=0)\n",
    "\n",
    "    labels_train_event1 = [0] * len(epochs_train_list_event1)\n",
    "    labels_train_event2 = [1] * len(epochs_train_list_event2)\n",
    "    y_train = np.concatenate((labels_train_event1, labels_train_event2), axis=0)\n",
    "\n",
    "    ######--------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    epochs_test_list_event1 = epochs_test[events[0]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    epochs_test_list_event2 = epochs_test[events[1]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    X_test = np.concatenate((epochs_test_list_event1, epochs_test_list_event2), axis=0)\n",
    "\n",
    "    labels_test_event1 = [0] * len(epochs_test_list_event1)\n",
    "    labels_test_event2 = [1] * len(epochs_test_list_event2)\n",
    "    y_test = np.concatenate((labels_test_event1, labels_test_event2), axis=0)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def get_X_and_Y_from_epochs_with_feature_extraction(train_list, test_list, events, picks=None, window_size = 0.02):\n",
    "\n",
    "    t_min = -0.2\n",
    "    t_max = 0.5\n",
    "\n",
    "    X_train, X_test, y_train, y_test = get_X_and_Y_from_epochs(train_list, test_list, events, picks, t_min, t_max)\n",
    "\n",
    "    times = np.linspace(t_min, t_max, X_train.shape[2])\n",
    "\n",
    "    # Feature exctraction from peaks\n",
    "    X_train_feats = extract_peak_features(X_train, times,window_size)\n",
    "    X_test_feats = extract_peak_features(X_test, times, window_size)\n",
    "\n",
    "    logging.info(f\"shape: {X_train_feats.shape}\")\n",
    "\n",
    "    return X_train_feats, X_test_feats, y_train, y_test\n",
    "\n",
    "def eval_split(name, X, y, model, condition_in_out):\n",
    "    y_pred = model.predict(X)\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "\n",
    "    logging.info(f\"\\n== {name.upper()} ==\")\n",
    "    logging.info(f\"AUC      : {roc_auc_score(y, model.predict_proba(X)[:, 1]):.4f}\")\n",
    "    logging.info(f\"Accuracy : {accuracy_score(y, y_pred):.4f}\")\n",
    "    logging.info(f\"F1       : {f1_score(y, y_pred, pos_label=1):.4f}\")\n",
    "    logging.info(f\"Precision: {precision_score(y, y_pred, pos_label=1):.4f}\")\n",
    "    logging.info(f\"Recall   : {recall_score(y, y_pred, pos_label=1):.4f}\")\n",
    "    if condition_in_out:\n",
    "        cm_df = pd.DataFrame(cm, index=[\"Actual in-group\", \"Actual out-group\"], columns=[\"Predicted in-group\", \"Predicted out-group\"])\n",
    "    else:\n",
    "        cm_df = pd.DataFrame(cm, index=[\"Actual inv\", \"Actual up\"], columns=[\"Predicted inv\", \"Predicted up\"])\n",
    "    logging.info(f\"\\nConfusion matrix ({name}):\")\n",
    "    logging.info(cm_df)\n",
    "\n",
    "\n",
    "def train_and_test_model(X_train, X_test, y_train, y_test, model, name, condition_in_out):\n",
    "\n",
    "    logging.info(f\"\\n== {name.upper()} ==\")\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    eval_split(\"train\", X_train, y_train, model, condition_in_out)\n",
    "    eval_split(\"test\",  X_test,  y_test, model, condition_in_out)\n",
    "\n",
    "    logging.info(\"\\n== GridSearchCV ==\")\n",
    "    logging.info(f\"Best params: {model.best_params_}\")\n",
    "\n",
    "    return model"
   ],
   "id": "535cf64518e20f81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dir_path = 'D:\\studia\\magisterka\\dane EEG\\BADANIE_POLITYCZNE_2022_eeg_bdfy\\EEG_preprocessed'\n",
    "file_name_template = \"s*.bdf-epo.fif\"\n",
    "train_ratio = 0.8\n",
    "selected_channels = ['P5', 'P6', 'P7', 'P8','PO7', 'PO8']\n",
    "\n",
    "flatten_transformer = FunctionTransformer(lambda X: X.reshape(X.shape[0], -1))"
   ],
   "id": "4654b6f52304e20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LOGGER",
   "id": "39432da8f78e2863"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "log_file = \"training_log.txt\"\n",
    "\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file, mode=\"a\"), # a to overwrite\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Wyłącz logi zewnętrznych bibliotek (MNE, sklearn, matplotlib, itp.)\n",
    "logging.getLogger(\"mne\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"sklearn\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"matplotlib\").setLevel(logging.ERROR)"
   ],
   "id": "e8bdff910ff3afe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## GBC pipeline & param grid",
   "id": "e0f1ad751b698f86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_gbc = Pipeline(steps=[\n",
    "    ('reshape', flatten_transformer),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('gbc', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "param_grid_gbc = {\n",
    "    'gbc__n_estimators': [100, 200]\n",
    "}"
   ],
   "id": "bf01279d7cd180ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LDA pipeline & param grid",
   "id": "57617aa3529dbdc1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_lda = Pipeline(steps=[\n",
    "    ('reshape', flatten_transformer),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lda', LinearDiscriminantAnalysis())\n",
    "])\n",
    "\n",
    "param_grid_lda = {\n",
    "    \"lda__solver\": [\"lsqr\"],\n",
    "    \"lda__shrinkage\": [0.2, 0.5, 0.7]\n",
    "}"
   ],
   "id": "76dca79e59cc2980",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MODEL 1: GBC IN/OUT Exploration",
   "id": "b62708eb7e3339b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_list, test_list = split_train_test_path_list(dir_path, file_name_template, train_ratio)\n",
    "X_train, X_test, y_train, y_test = get_X_and_Y_from_epochs(train_list, test_list, [\"in\", \"out\"],  selected_channels, t_min=0.1, t_max=0.25)\n",
    "model_1 = GridSearchCV(model_gbc, param_grid_gbc,  scoring=make_scorer(roc_auc_score, needs_threshold=True), cv=5)\n",
    "\n",
    "train_and_test_model(X_train, X_test, y_train, y_test, model_1, \"GBC IN/OUT EXPLORATION\", condition_in_out = True)"
   ],
   "id": "6bbf2c1d8ce4a55f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MODEL 2: GBC IN/OUT Feature Extraction",
   "id": "36bca4c453c59427"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_gbc = Pipeline(steps=[\n",
    "    ('reshape', flatten_transformer),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('gbc', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'gbc__n_estimators': [100, 200]\n",
    "}\n",
    "\n",
    "train_list, test_list = split_train_test_path_list(dir_path, file_name_template, train_ratio)\n",
    "X_train, X_test, y_train, y_test = get_X_and_Y_from_epochs_with_feature_extraction(train_list, test_list, [\"in\", \"out\"],  selected_channels)\n",
    "model_2 = GridSearchCV(model_gbc, param_grid,  scoring=\"roc_auc\", cv=5)\n",
    "\n",
    "train_and_test_model(X_train, X_test, y_train, y_test, model_2, \"GBC IN/OUT Feature Extraction\", condition_in_out = True)"
   ],
   "id": "9d6ddeddc603a9de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MODEL 3: LDA IN/OUT Exploration",
   "id": "92dadbc87823cd1b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_list, test_list = split_train_test_path_list(dir_path, file_name_template, train_ratio)\n",
    "X_train, X_test, y_train, y_test = get_X_and_Y_from_epochs(train_list, test_list, [\"in\", \"out\"], selected_channels, t_min=0.1, t_max=0.25)\n",
    "model_3 = GridSearchCV(model_lda, param_grid_lda,  scoring=\"roc_auc\", cv=5)\n",
    "\n",
    "train_and_test_model(X_train, X_test, y_train, y_test, model_3, \"LDA IN/OUT Exploration\", condition_in_out = True)"
   ],
   "id": "634eaf8557c36402",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MODEL 4: LDA IN/OUT Feature Extraction",
   "id": "f9d143806e71205f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_list, test_list = split_train_test_path_list(dir_path, file_name_template, train_ratio)\n",
    "X_train, X_test, y_train, y_test = get_X_and_Y_from_epochs_with_feature_extraction(train_list, test_list, [\"in\", \"out\"],  selected_channels)\n",
    "model_4 = GridSearchCV(model_lda, param_grid_lda,  scoring=\"roc_auc\", cv=5)\n",
    "\n",
    "train_and_test_model(X_train, X_test, y_train, y_test, model_4, \"LDA IN/OUT Feature Extraction\", condition_in_out = True)"
   ],
   "id": "6118c0748602d14e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MODEL 5: GBC INV/UP Exploration\n",
   "id": "11985cb6e15cabc1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_list, test_list = split_train_test_path_list(dir_path, file_name_template, train_ratio)\n",
    "X_train, X_test, y_train, y_test = get_X_and_Y_from_epochs(train_list, test_list, [\"inv\", \"up\"],  selected_channels, t_min=0.1, t_max=0.25)\n",
    "model_5 = GridSearchCV(model_gbc, param_grid_gbc,  scoring=\"roc_auc\", cv=5)\n",
    "\n",
    "train_and_test_model(X_train, X_test, y_train, y_test, model_5, \"GBC INV/UP EXPLORATION\", condition_in_out = False)"
   ],
   "id": "810dac782ef03a96",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MODEL 6: GBC INV/UP Feature Extraction",
   "id": "ab727b7ca5f36d5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_list, test_list = split_train_test_path_list(dir_path, file_name_template, train_ratio)\n",
    "X_train, X_test, y_train, y_test = get_X_and_Y_from_epochs_with_feature_extraction(train_list, test_list, [\"inv\", \"up\"],  selected_channels)\n",
    "model_6 = GridSearchCV(model_gbc, param_grid_gbc,  scoring=\"roc_auc\", cv=5)\n",
    "\n",
    "train_and_test_model(X_train, X_test, y_train, y_test, model_6, \"GBC INV/UP Feature Extraction\", condition_in_out = False)"
   ],
   "id": "c85a35c4267c3118",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MODEL 7: LDA INV/UP Exploration",
   "id": "88b668c49abffbd8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_list, test_list = split_train_test_path_list(dir_path, file_name_template, train_ratio)\n",
    "X_train, X_test, y_train, y_test = get_X_and_Y_from_epochs(train_list, test_list, [\"inv\", \"up\"], selected_channels, t_min=0.1, t_max=0.25)\n",
    "model_7 = GridSearchCV(model_lda, param_grid_lda,  scoring=\"roc_auc\", cv=5)\n",
    "\n",
    "train_and_test_model(X_train, X_test, y_train, y_test, model_7, \"LDA INV/UP Exploration\", condition_in_out = False)"
   ],
   "id": "581d99b8a805f27e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MODEL 8: LDA INV/UP Feature Extraction",
   "id": "38effa8d952cfbc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_list, test_list = split_train_test_path_list(dir_path, file_name_template, train_ratio)\n",
    "X_train, X_test, y_train, y_test = get_X_and_Y_from_epochs_with_feature_extraction(train_list, test_list, [\"inv\", \"up\"],\n",
    "                                                                                   selected_channels)\n",
    "model_8 = GridSearchCV(model_lda, param_grid_lda, scoring=\"roc_auc\", cv=5)\n",
    "\n",
    "train_and_test_model(X_train, X_test, y_train, y_test, model_8, \"LDA INV/UP Feature Extraction\", condition_in_out=False)"
   ],
   "id": "64cb285c4ecc1208",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Subsample average",
   "id": "794a08e3497b1db0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def subsample_average(data, subsample_size = 5):\n",
    "    data_copy = data.copy()\n",
    "    averaged_data = []\n",
    "\n",
    "    while len(data_copy) >= subsample_size:\n",
    "        indices = np.random.choice(len(data_copy), subsample_size, replace=False)\n",
    "\n",
    "        selected = data_copy[indices]\n",
    "        averaged = np.mean(selected, axis=0)\n",
    "        averaged_data.append(averaged)\n",
    "\n",
    "        mask = np.ones(len(data_copy), dtype=bool)\n",
    "        mask[indices] = False\n",
    "        data_copy = data_copy[mask]\n",
    "\n",
    "    return np.array(averaged_data)\n",
    "\n",
    "\n",
    "def get_X_and_Y_from_epochs_subsample_averaging(train_list, test_list, events, picks=None, t_min = -0.2, t_max = 0.5, subsample_size = 5):\n",
    "\n",
    "    epochs_train, epochs_test = read_eeg_epochs(train_list, test_list)\n",
    "\n",
    "    #####---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    epochs_train_list_event1 = epochs_train[events[0]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    subsample_average_train_list_event1 = subsample_average(epochs_train_list_event1, subsample_size=subsample_size)\n",
    "    epochs_train_list_event2 = epochs_train[events[1]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    subsample_average_train_list_event2 = subsample_average(epochs_train_list_event2, subsample_size=subsample_size)\n",
    "    X_train = np.concatenate((subsample_average_train_list_event1, subsample_average_train_list_event2), axis=0)\n",
    "\n",
    "    labels_up_train = [0] * len(subsample_average_train_list_event1)\n",
    "    labels_inv_train = [1] * len(subsample_average_train_list_event2)\n",
    "    y_train = np.concatenate((labels_up_train, labels_inv_train), axis=0)\n",
    "\n",
    "    ######--------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    epochs_test_list_event1 = epochs_test[events[0]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    subsample_average_test_list_event1 = subsample_average(epochs_test_list_event1, subsample_size=subsample_size)\n",
    "    epochs_test_list_event2 = epochs_test[events[1]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    subsample_average_test_list_event2 = subsample_average(epochs_test_list_event2, subsample_size=subsample_size)\n",
    "    X_test = np.concatenate((subsample_average_test_list_event1, subsample_average_test_list_event2), axis=0)\n",
    "\n",
    "    labels_up_test = [0] * len(subsample_average_test_list_event1)\n",
    "    labels_inv_test = [1] * len(subsample_average_test_list_event2)\n",
    "    y_test = np.concatenate((labels_up_test, labels_inv_test), axis=0)\n",
    "\n",
    "    logging.info(f\"shape: {X_train.shape}\")\n",
    "\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ],
   "id": "ae70d6bf1f30bfab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## GBC IN OUT EXPLORATION SUBSAMPLE AVERAGE",
   "id": "c49ac9855b184a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_list, test_list = split_train_test_path_list(dir_path, file_name_template, train_ratio)\n",
    "X_train, X_test, y_train, y_test = get_X_and_Y_from_epochs_subsample_averaging(train_list, test_list, [\"in\", \"out\"],  selected_channels, t_min=0.1, t_max=0.25, subsample_size=3)\n",
    "model_subsample = GridSearchCV(model_gbc, param_grid_gbc,  scoring=\"roc_auc\", cv=5)\n",
    "\n",
    "train_and_test_model(X_train, X_test, y_train, y_test, model_subsample, \"GBC IN/OUT EXPLORATION SUBSAMPLE AVERAGE = 3\", condition_in_out = True)"
   ],
   "id": "c64f321c04f4e787",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## GBC INV/UP EXPLORATION SUBSAMPLE AVERAGE",
   "id": "9bf40b02e3acf11c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_list, test_list = split_train_test_path_list(dir_path, file_name_template, train_ratio)\n",
    "X_train, X_test, y_train, y_test = get_X_and_Y_from_epochs_subsample_averaging(train_list, test_list, [\"inv\", \"up\"],  selected_channels, t_min=0.1, t_max=0.25, subsample_size=3)\n",
    "model_subsample2 = GridSearchCV(model_gbc, param_grid_gbc, scoring=\"roc_auc\", cv=5)\n",
    "\n",
    "train_and_test_model(X_train, X_test, y_train, y_test, model_subsample2, \"GBC INV/UP EXPLORATION SUBSAMPLE AVERAGE = 3\", condition_in_out = False)"
   ],
   "id": "770f8882da5399d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LDA IN/OUT EXPLORATION SUBSAMPLE AVERAGE",
   "id": "b379b0b05a9f80c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_list, test_list = split_train_test_path_list(dir_path, file_name_template, train_ratio)\n",
    "X_train, X_test, y_train, y_test = get_X_and_Y_from_epochs_subsample_averaging(train_list, test_list, [\"in\", \"out\"],\n",
    "                                                                               selected_channels, t_min=0.1, t_max=0.25,\n",
    "                                                                               subsample_size=5)\n",
    "model_subsample3 = GridSearchCV(model_lda, param_grid_lda, scoring=\"roc_auc\", cv=5)\n",
    "\n",
    "train_and_test_model(X_train, X_test, y_train, y_test, model_subsample3, \"LDA IN/OUT EXPLORATION SUBSAMPLE AVERAGE = 5\",\n",
    "                     condition_in_out=True)"
   ],
   "id": "42994e581c3b0fc5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LDA INV/UP EXPLORATION SUBSAMPLE AVERAGE",
   "id": "eb1a98e597a578b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_list, test_list = split_train_test_path_list(dir_path, file_name_template, train_ratio)\n",
    "X_train, X_test, y_train, y_test = get_X_and_Y_from_epochs_subsample_averaging(train_list, test_list, [\"inv\", \"up\"],  selected_channels, t_min=0.1, t_max=0.25, subsample_size=3)\n",
    "model_subsample4 = GridSearchCV(model_lda, param_grid_lda, scoring=\"roc_auc\", cv=5)\n",
    "\n",
    "train_and_test_model(X_train, X_test, y_train, y_test, model_subsample4, \"LDA INV/UP EXPLORATION SUBSAMPLE AVERAGE = 5\", condition_in_out = False)"
   ],
   "id": "e4980d8623c8d838",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
