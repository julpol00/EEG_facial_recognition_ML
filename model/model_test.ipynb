{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-17T19:14:38.421662Z",
     "start_time": "2025-08-17T19:14:36.516543Z"
    }
   },
   "source": [
    "import sklearn\n",
    "import mne\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from joblib import parallel_backend\n",
    "import multiprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from pywt import wavedec"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T19:14:38.868691Z",
     "start_time": "2025-08-17T19:14:38.849590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def split_train_test_path_list(data_path, file_name_template, train_ratio):\n",
    "    file_list = sorted(glob.glob(os.path.join(data_path, file_name_template)))\n",
    "    np.random.shuffle(file_list)\n",
    "    split_id = int(len(file_list) * train_ratio)\n",
    "\n",
    "    train_list = file_list[:split_id]\n",
    "    test_list = file_list[split_id:]\n",
    "\n",
    "    return train_list, test_list\n"
   ],
   "id": "43b7b52a181a65b3",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T19:14:39.917960Z",
     "start_time": "2025-08-17T19:14:39.898865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def read_all_epochs(data_path, file_name_template, events, picks=None, t_min = -0.2, t_max = 0.5):\n",
    "    file_list = sorted(glob.glob(os.path.join(data_path, file_name_template)))\n",
    "    np.random.shuffle(file_list)\n",
    "\n",
    "    epochs_list = []\n",
    "    for file_path in train_list:\n",
    "        epoch = mne.read_epochs(file_path, preload=True)\n",
    "        epochs_list.append(epoch)\n",
    "\n",
    "    epochs_list = mne.concatenate_epochs(epochs_list)\n",
    "\n",
    "    epochs_event_1 = epochs_list[events[0]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    epochs_event_2 = epochs_list[events[1]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "\n",
    "    return epochs_event_1, epochs_event_2"
   ],
   "id": "73ffe9a5f68e741e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T19:14:41.112618Z",
     "start_time": "2025-08-17T19:14:41.096470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def read_eeg_epochs(train_list, test_list):\n",
    "    epochs_train_list = []\n",
    "    epochs_test_list = []\n",
    "\n",
    "    for file_path in train_list:\n",
    "        with mne.utils.use_log_level(\"ERROR\"):\n",
    "            epoch_train = mne.read_epochs(file_path, preload=True)\n",
    "            epochs_train_list.append(epoch_train)\n",
    "\n",
    "    for file_path in test_list:\n",
    "        with mne.utils.use_log_level(\"ERROR\"):\n",
    "            epoch_test = mne.read_epochs(file_path, preload=True)\n",
    "            epochs_test_list.append(epoch_test)\n",
    "\n",
    "    epochs_train = mne.concatenate_epochs(epochs_train_list)\n",
    "    epochs_test = mne.concatenate_epochs(epochs_test_list)\n",
    "\n",
    "    return epochs_train, epochs_test"
   ],
   "id": "f2c4cb8c973494e7",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T19:01:16.556355Z",
     "start_time": "2025-08-17T19:01:16.544341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([5, 2, 8, -3, 7])\n",
    "idx = np.min(a)\n",
    "print(idx)         # => 3\n",
    "print(a[idx])"
   ],
   "id": "64ab38f0b687e31a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3\n",
      "8\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T22:02:39.929255Z",
     "start_time": "2025-07-20T22:02:39.619637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scipy.signal import find_peaks\n",
    "import numpy as np\n",
    "\n",
    "y = np.array([0, 2, 1, 3, 0, 1, 2, 1])\n",
    "peaks, _ = find_peaks(y)\n",
    "print(peaks)        # => [1 3 6]\n",
    "print(y[peaks])"
   ],
   "id": "dec713e256a8091c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 6]\n",
      "[2 3 2]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T19:42:24.469608Z",
     "start_time": "2025-08-17T19:42:24.435607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "def get_window_idx(center, width, times_arr):\n",
    "    return np.where((times_arr >= center - width/2) & (times_arr <= center + width/2))[0]\n",
    "\n",
    "def zero_crossings(sig):\n",
    "    return np.where(np.diff(np.sign(sig)))[0].size\n",
    "\n",
    "def extract_peak_features(X, times, window_size=0.04, baseline_correction = False):\n",
    "\n",
    "    n_epochs, n_channels, n_times = X.shape\n",
    "    features = []\n",
    "\n",
    "    peak_window_N170 =(0.12, 0.22)\n",
    "    peak_window_P100=(0.05, 0.15)\n",
    "    peak_window_P300 = (0.2, 0.3)\n",
    "\n",
    "    win_mask_N170 = (times >= peak_window_N170[0]) & (times <= peak_window_N170[1])\n",
    "    win_mask_P100 = (times >= peak_window_P100[0]) & (times <= peak_window_P100[1])\n",
    "    win_mask_P300 = (times >= peak_window_P300[0]) & (times <= peak_window_P300[1])\n",
    "\n",
    "    for ep in range(n_epochs):\n",
    "        feats_ep = []\n",
    "        for ch in range(n_channels):\n",
    "            signal = X[ep, ch, :]\n",
    "\n",
    "\n",
    "            sig_win_P100 = signal[win_mask_P100]\n",
    "            times_win_P100 = times[win_mask_P100]\n",
    "\n",
    "            peaks, _ = find_peaks(sig_win_P100, distance=100)\n",
    "            if len(peaks) == 0:\n",
    "                idx1 = np.argmax(sig_win_P100)\n",
    "            else:\n",
    "                idx1 = peaks[0]\n",
    "            time1 = times_win_P100[idx1]\n",
    "            amp1 = float(sig_win_P100[idx1])\n",
    "            if baseline_correction:\n",
    "                amp1 -= float(sig_win_P100[0])\n",
    "\n",
    "\n",
    "            sig_win_N170 = signal[win_mask_N170]\n",
    "            times_win_N170 = times[win_mask_N170]\n",
    "\n",
    "            peaks_min, _ = find_peaks(-sig_win_N170, distance=100)\n",
    "            if len(peaks_min) == 0:\n",
    "                idx2 = np.argmin(sig_win_N170)\n",
    "            else:\n",
    "                idx2 = peaks_min[0]\n",
    "            time2 = times_win_N170[idx2]\n",
    "            amp2 = float(sig_win_N170[idx2])\n",
    "            if baseline_correction:\n",
    "                amp2 -= float(sig_win_N170[0])\n",
    "\n",
    "\n",
    "            sig_win_P300 = signal[win_mask_P300]\n",
    "            times_win_P300 = times[win_mask_P300]\n",
    "\n",
    "            peaks, _ = find_peaks(sig_win_P300, distance=100)\n",
    "            if len(peaks) == 0:\n",
    "                idx3 = np.argmax(sig_win_P300)\n",
    "            else:\n",
    "                idx3 = peaks[0]\n",
    "            time3 = times_win_P300[idx3]\n",
    "            amp3 = float(sig_win_P300[idx3])\n",
    "            if baseline_correction:\n",
    "                amp3 -= float(sig_win_P300[0])\n",
    "\n",
    "            win1 = get_window_idx(time1, window_size, times)\n",
    "            win2 = get_window_idx(time2, window_size, times)\n",
    "            win3 = get_window_idx(time3, window_size, times)\n",
    "\n",
    "            feats_ep.extend([time1, time2, time3, amp1, amp2, amp3])\n",
    "\n",
    "            for win in [win1, win2, win3]:\n",
    "                if len(win) > 0:\n",
    "                    sig_win = signal[win]\n",
    "                    t_win = times[win]\n",
    "\n",
    "                    # 7. Zero-crossing rate\n",
    "                    feat_zc = zero_crossings(sig_win)\n",
    "                    # 8. Peak-to-peak amplitude\n",
    "                    feat_ptp = np.ptp(sig_win)\n",
    "\n",
    "                    if baseline_correction:\n",
    "                        sig_win = sig_win - sig_win[0]\n",
    "\n",
    "                    # Feature calculations:\n",
    "                    # 1. RMS\n",
    "                    feat_rms = np.sqrt(np.mean(sig_win ** 2))\n",
    "                    # 2. Variance\n",
    "                    feat_var = np.var(sig_win)\n",
    "                    # 3. Std\n",
    "                    feat_std = np.std(sig_win)\n",
    "                    # 4. Skewness\n",
    "                    feat_skew = skew(sig_win)\n",
    "                    # 5. Kurtosis\n",
    "                    feat_kurt = kurtosis(sig_win)\n",
    "                    # 6. Area under the curve\n",
    "                    feat_auc = np.sum(sig_win)\n",
    "                    # 9. Slope\n",
    "                    # Fit line: polyfit returns [slope, intercept]\n",
    "                    slope = np.polyfit(t_win, sig_win, 1)[0] if len(sig_win) > 1 else np.nan\n",
    "                    # 10. Mean\n",
    "                    feat_mean = np.mean(sig_win)\n",
    "                    # 11. Min\n",
    "                    feat_min = np.min(sig_win)\n",
    "                    # 12. Max\n",
    "                    feat_max = np.max(sig_win)\n",
    "                    # 13. Median\n",
    "                    feat_median = np.median(sig_win)\n",
    "\n",
    "                    feats_ep.extend([\n",
    "                        feat_rms, feat_var, feat_std, feat_skew, feat_kurt, feat_auc, feat_zc, feat_ptp, slope, feat_mean, feat_min, feat_max, feat_median\n",
    "                    ])\n",
    "                else:\n",
    "                    feats_ep.extend([np.nan]*14)\n",
    "\n",
    "        features.append(feats_ep)\n",
    "    return np.array(features)  # shape: (n_epochs, n_channels*3*14)\n"
   ],
   "id": "1f8c56410f507cfb",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T19:14:48.261884Z",
     "start_time": "2025-08-17T19:14:48.245790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_n170_features(X, times, n170_window=(0.12, 0.22)):\n",
    "    \"\"\"\n",
    "    X: array (n_epochs, n_channels, n_times)\n",
    "    times: array (n_times,) w sekundach\n",
    "    n170_window: tuple (start, end) w sekundach\n",
    "    Zwracane cechy (per kanał):\n",
    "      [ min_amp_N170, latency_N170, mean_N170 ]\n",
    "    \"\"\"\n",
    "    n_epochs, n_channels, _ = X.shape\n",
    "    win_mask = (times >= n170_window[0]) & (times <= n170_window[1])\n",
    "    feats = np.full((n_epochs, n_channels, 3), np.nan, dtype=float)\n",
    "\n",
    "    if not np.any(win_mask):\n",
    "        return feats.reshape(n_epochs, -1)\n",
    "\n",
    "    t_win = times[win_mask]\n",
    "\n",
    "    for ep in range(n_epochs):\n",
    "        for ch in range(n_channels):\n",
    "            sig = X[ep, ch, :]\n",
    "            sig_win = sig[win_mask]\n",
    "\n",
    "            if sig_win.size == 0:\n",
    "                continue  # zostają NaNy\n",
    "\n",
    "            idx_min = int(np.argmin(sig_win))\n",
    "            min_amp = float(sig_win[idx_min])\n",
    "            latency = float(t_win[idx_min])\n",
    "\n",
    "            mean_amp = float(np.mean(sig_win))\n",
    "\n",
    "            feats[ep, ch, 0] = min_amp\n",
    "            feats[ep, ch, 1] = latency\n",
    "            feats[ep, ch, 2] = mean_amp\n",
    "\n",
    "    return feats"
   ],
   "id": "239fd8bb89909fb7",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T19:01:27.169116Z",
     "start_time": "2025-08-17T19:01:27.155115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def band_power(sig, fs, band):\n",
    "    fft_vals = np.fft.rfft(sig)\n",
    "    power = np.abs(fft_vals) ** 2\n",
    "    freqs = np.fft.rfftfreq(len(sig), 1/fs)\n",
    "    idx = np.where((freqs >= band[0]) & (freqs < band[1]))[0]\n",
    "    return np.sum(power[idx])\n",
    "\n",
    "def extract_band_power_features(X, times, fs=128):\n",
    "    n_epochs, n_channels, n_times = X.shape\n",
    "    features = []\n",
    "\n",
    "    window = (0.12, 0.22)  # 120–220 ms\n",
    "    win_mask = (times >= window[0]) & (times <= window[1])\n",
    "\n",
    "    bands = {\n",
    "        'delta': (1, 4),\n",
    "        'theta': (4, 8),\n",
    "        'alpha': (8, 13),\n",
    "        'beta': (13, 30),\n",
    "        'gamma': (30, 45)\n",
    "    }\n",
    "    band_names = list(bands.keys())\n",
    "\n",
    "    for ep in range(n_epochs):\n",
    "        feats_ep = []\n",
    "        for ch in range(n_channels):\n",
    "            sig_win = X[ep, ch, win_mask]\n",
    "            if len(sig_win) < 2: # if window is empty  fullfill with NaN\n",
    "                feats_ep.extend([np.nan]*len(bands))\n",
    "                continue\n",
    "            for band in bands.values():\n",
    "                bp = band_power(sig_win, fs, band)\n",
    "                feats_ep.append(bp)\n",
    "        features.append(feats_ep)\n",
    "    return np.array(features)  # shape: (n_epochs, n_channels*len(bands))\n"
   ],
   "id": "e6cc19a93725e545",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T19:01:29.743437Z",
     "start_time": "2025-08-17T19:01:29.733437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_X_and_Y_from_epochs(train_list, test_list, events, picks=None, t_min = -0.2, t_max = 0.5):\n",
    "\n",
    "    epochs_train, epochs_test = read_eeg_epochs(train_list, test_list)\n",
    "\n",
    "    #####---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    epochs_train_list_event1 = epochs_train[events[0]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    epochs_train_list_event2 = epochs_train[events[1]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    X_train = np.concatenate((epochs_train_list_event1, epochs_train_list_event2), axis=0)\n",
    "\n",
    "    labels_up_train = [0] * len(epochs_train_list_event1)\n",
    "    labels_inv_train = [1] * len(epochs_train_list_event2)\n",
    "    y_train = np.concatenate((labels_up_train, labels_inv_train), axis=0)\n",
    "\n",
    "    ######--------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    epochs_test_list_event1 = epochs_test[events[0]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    epochs_test_list_event2 = epochs_test[events[1]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    X_test = np.concatenate((epochs_test_list_event1, epochs_test_list_event2), axis=0)\n",
    "\n",
    "    labels_up_test = [0] * len(epochs_test_list_event1)\n",
    "    labels_inv_test = [1] * len(epochs_test_list_event2)\n",
    "    y_test = np.concatenate((labels_up_test, labels_inv_test), axis=0)\n",
    "\n",
    "    logging.info(f\"shape: {X_train.shape}\")\n",
    "\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ],
   "id": "a7cf908c72c64b57",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T19:01:31.271351Z",
     "start_time": "2025-08-17T19:01:31.257354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def subsample_average(data, subsample_size = 5):\n",
    "    data_copy = data.copy()\n",
    "    averaged_data = []\n",
    "\n",
    "    while len(data_copy) >= subsample_size:\n",
    "        indices = np.random.choice(len(data_copy), subsample_size, replace=False)\n",
    "\n",
    "        selected = data_copy[indices]\n",
    "        averaged = np.mean(selected, axis=0)\n",
    "        averaged_data.append(averaged)\n",
    "\n",
    "        mask = np.ones(len(data_copy), dtype=bool)\n",
    "        mask[indices] = False\n",
    "        data_copy = data_copy[mask]\n",
    "\n",
    "    return np.array(averaged_data)\n"
   ],
   "id": "a74857fbabddd9fb",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T19:14:55.559551Z",
     "start_time": "2025-08-17T19:14:55.541405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_X_and_Y_from_epochs_with_base_feature_extraction(train_list, test_list, events, picks=None):\n",
    "\n",
    "    t_min = -0.2\n",
    "    t_max = 0.5\n",
    "    epochs_train, epochs_test = read_eeg_epochs(train_list, test_list)\n",
    "\n",
    "    #####---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    epochs_train_list_event1 = epochs_train[events[0]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    epochs_train_list_event2 = epochs_train[events[1]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    X_train = np.concatenate((epochs_train_list_event1, epochs_train_list_event2), axis=0)\n",
    "\n",
    "    labels_up_train = [0] * len(epochs_train_list_event1)\n",
    "    labels_inv_train = [1] * len(epochs_train_list_event2)\n",
    "    y_train = np.concatenate((labels_up_train, labels_inv_train), axis=0)\n",
    "\n",
    "    ######--------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    epochs_test_list_event1 = epochs_test[events[0]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    epochs_test_list_event2 = epochs_test[events[1]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    X_test = np.concatenate((epochs_test_list_event1, epochs_test_list_event2), axis=0)\n",
    "\n",
    "    labels_up_test = [0] * len(epochs_test_list_event1)\n",
    "    labels_inv_test = [1] * len(epochs_test_list_event2)\n",
    "    y_test = np.concatenate((labels_up_test, labels_inv_test), axis=0)\n",
    "\n",
    "    #####---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    times = np.linspace(t_min, t_max, X_train.shape[2])\n",
    "    # Ekstrakcja cech peaków\n",
    "    X_train_feats = extract_n170_features(X_train, times)\n",
    "    X_test_feats = extract_n170_features(X_test, times)\n",
    "\n",
    "    logging.info(f\"shape: {X_train_feats.shape}\")\n",
    "\n",
    "    return X_train_feats, X_test_feats, y_train, y_test"
   ],
   "id": "dcb546cdddd0f338",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T17:41:24.759260Z",
     "start_time": "2025-07-21T17:41:24.746258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_X_and_Y_from_epochs_subsample_averaging_with_peak_feature_extraction(train_list, test_list, events, picks=None, t_min = -0.2, t_max = 0.5, subsample_size = 5):\n",
    "\n",
    "    epochs_train, epochs_test = read_eeg_epochs(train_list, test_list)\n",
    "\n",
    "    #####---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    epochs_train_list_event1 = epochs_train[events[0]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    subsample_average_train_list_event1 = subsample_average(epochs_train_list_event1, subsample_size=subsample_size)\n",
    "    epochs_train_list_event2 = epochs_train[events[1]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    subsample_average_train_list_event2 = subsample_average(epochs_train_list_event2, subsample_size=subsample_size)\n",
    "    X_train = np.concatenate((subsample_average_train_list_event1, subsample_average_train_list_event2), axis=0)\n",
    "\n",
    "    labels_up_train = [0] * len(subsample_average_train_list_event1)\n",
    "    labels_inv_train = [1] * len(subsample_average_train_list_event2)\n",
    "    y_train = np.concatenate((labels_up_train, labels_inv_train), axis=0)\n",
    "\n",
    "    ######--------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    epochs_test_list_event1 = epochs_test[events[0]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    subsample_average_test_list_event1 = subsample_average(epochs_test_list_event1, subsample_size=subsample_size)\n",
    "    epochs_test_list_event2 = epochs_test[events[1]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    subsample_average_test_list_event2 = subsample_average(epochs_test_list_event2, subsample_size=subsample_size)\n",
    "    X_test = np.concatenate((subsample_average_test_list_event1, subsample_average_test_list_event2), axis=0)\n",
    "\n",
    "    labels_up_test = [0] * len(subsample_average_test_list_event1)\n",
    "    labels_inv_test = [1] * len(subsample_average_test_list_event2)\n",
    "    y_test = np.concatenate((labels_up_test, labels_inv_test), axis=0)\n",
    "\n",
    "##### ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    times = np.linspace(t_min, t_max, X_train.shape[2])\n",
    "    # Ekstrakcja cech peaków\n",
    "    X_train_feats = extract_peak_features(X_train, times)\n",
    "    X_test_feats = extract_peak_features(X_test, times)\n",
    "\n",
    "    logging.info(f\"shape: {X_train_feats.shape}\")\n",
    "\n",
    "    return X_train_feats, X_test_feats, y_train, y_test"
   ],
   "id": "51df2351696096e4",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T20:20:25.771539Z",
     "start_time": "2025-08-17T20:20:25.746547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_X_and_Y_from_epochs_subsample_averaging(train_list, test_list, events, picks=None, t_min = -0.2, t_max = 0.5, subsample_size = 5):\n",
    "\n",
    "    epochs_train, epochs_test = read_eeg_epochs(train_list, test_list)\n",
    "\n",
    "    #####---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    epochs_train_list_event1 = epochs_train[events[0]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    subsample_average_train_list_event1 = subsample_average(epochs_train_list_event1, subsample_size=subsample_size)\n",
    "    epochs_train_list_event2 = epochs_train[events[1]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    subsample_average_train_list_event2 = subsample_average(epochs_train_list_event2, subsample_size=subsample_size)\n",
    "    X_train = np.concatenate((subsample_average_train_list_event1, subsample_average_train_list_event2), axis=0)\n",
    "\n",
    "    labels_up_train = [0] * len(subsample_average_train_list_event1)\n",
    "    labels_inv_train = [1] * len(subsample_average_train_list_event2)\n",
    "    y_train = np.concatenate((labels_up_train, labels_inv_train), axis=0)\n",
    "\n",
    "    ######--------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    epochs_test_list_event1 = epochs_test[events[0]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    subsample_average_test_list_event1 = subsample_average(epochs_test_list_event1, subsample_size=subsample_size)\n",
    "    epochs_test_list_event2 = epochs_test[events[1]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    subsample_average_test_list_event2 = subsample_average(epochs_test_list_event2, subsample_size=subsample_size)\n",
    "    X_test = np.concatenate((subsample_average_test_list_event1, subsample_average_test_list_event2), axis=0)\n",
    "\n",
    "    labels_up_test = [0] * len(subsample_average_test_list_event1)\n",
    "    labels_inv_test = [1] * len(subsample_average_test_list_event2)\n",
    "    y_test = np.concatenate((labels_up_test, labels_inv_test), axis=0)\n",
    "\n",
    "    logging.info(f\"shape: {X_train.shape}\")\n",
    "\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ],
   "id": "eca775de8b31d90a",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T19:43:27.970011Z",
     "start_time": "2025-08-17T19:43:27.955015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_X_and_Y_from_epochs_with_feature_extraction(train_list, test_list, events, picks=None, window_size = 0.02, baseline_correction = False):\n",
    "\n",
    "    t_min = -0.2\n",
    "    t_max = 0.5\n",
    "    epochs_train, epochs_test = read_eeg_epochs(train_list, test_list)\n",
    "\n",
    "    #####---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    epochs_train_list_event1 = epochs_train[events[0]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    epochs_train_list_event2 = epochs_train[events[1]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    X_train = np.concatenate((epochs_train_list_event1, epochs_train_list_event2), axis=0)\n",
    "\n",
    "    labels_up_train = [0] * len(epochs_train_list_event1)\n",
    "    labels_inv_train = [1] * len(epochs_train_list_event2)\n",
    "    y_train = np.concatenate((labels_up_train, labels_inv_train), axis=0)\n",
    "\n",
    "    ######--------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    epochs_test_list_event1 = epochs_test[events[0]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    epochs_test_list_event2 = epochs_test[events[1]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    X_test = np.concatenate((epochs_test_list_event1, epochs_test_list_event2), axis=0)\n",
    "\n",
    "    labels_up_test = [0] * len(epochs_test_list_event1)\n",
    "    labels_inv_test = [1] * len(epochs_test_list_event2)\n",
    "    y_test = np.concatenate((labels_up_test, labels_inv_test), axis=0)\n",
    "\n",
    "    #####---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    times = np.linspace(t_min, t_max, X_train.shape[2])\n",
    "    # Ekstrakcja cech peaków\n",
    "    X_train_feats = extract_peak_features(X_train, times,window_size, baseline_correction)\n",
    "    X_test_feats = extract_peak_features(X_test, times, window_size, baseline_correction)\n",
    "\n",
    "    logging.info(f\"shape: {X_train_feats.shape}\")\n",
    "\n",
    "    return X_train_feats, X_test_feats, y_train, y_test"
   ],
   "id": "bba0286fb285c846",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T17:51:06.347213Z",
     "start_time": "2025-07-21T17:51:06.334213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_X_and_Y_from_epochs_with_PCA(train_list, test_list, events, picks=None):\n",
    "\n",
    "    t_min = -0.2\n",
    "    t_max = 0.5\n",
    "    epochs_train, epochs_test = read_eeg_epochs(train_list, test_list)\n",
    "\n",
    "    #####---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    epochs_train_list_event1 = epochs_train[events[0]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    epochs_train_list_event2 = epochs_train[events[1]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    X_train = np.concatenate((epochs_train_list_event1, epochs_train_list_event2), axis=0)\n",
    "\n",
    "    labels_up_train = [0] * len(epochs_train_list_event1)\n",
    "    labels_inv_train = [1] * len(epochs_train_list_event2)\n",
    "    y_train = np.concatenate((labels_up_train, labels_inv_train), axis=0)\n",
    "\n",
    "    ######--------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    epochs_test_list_event1 = epochs_test[events[0]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    epochs_test_list_event2 = epochs_test[events[1]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    X_test = np.concatenate((epochs_test_list_event1, epochs_test_list_event2), axis=0)\n",
    "\n",
    "    labels_up_test = [0] * len(epochs_test_list_event1)\n",
    "    labels_inv_test = [1] * len(epochs_test_list_event2)\n",
    "    y_test = np.concatenate((labels_up_test, labels_inv_test), axis=0)\n",
    "\n",
    "    #####---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    pca = PCA(n_components=0.9)\n",
    "    X_train_reshaped = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test_reshaped = X_test.reshape(X_test.shape[0], -1)\n",
    "    X_train_pca = pca.fit_transform(X_train_reshaped)\n",
    "    X_test_pca = pca.transform(X_test_reshaped)\n",
    "\n",
    "    print(\"Ilość komponentów PCA:\", pca.n_components_)\n",
    "\n",
    "    logging.info(f\"shape: {X_train_pca.shape}\")\n",
    "\n",
    "    return X_train_pca, X_test_pca, y_train, y_test"
   ],
   "id": "19d405a9eaa4f9ee",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T19:07:10.218483Z",
     "start_time": "2025-08-17T19:07:10.205322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def wavelet_avg_features(data, type_wav='db4'):\n",
    "\n",
    "    n_epochs, n_channels, n_times = data.shape\n",
    "    all_features = []\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        coeffs = wavedec(data, type_wav,level=3)\n",
    "        features = []\n",
    "        for ch in range(n_channels):\n",
    "            cD_Energy = np.mean([\n",
    "                             np.sum(np.square(coeffs[3])),np.sum(np.square(coeffs[2])),\n",
    "                             np.sum(np.square(coeffs[1]))])\n",
    "            cA_Energy = np.sum(np.square(coeffs[0]))\n",
    "            D_Entropy = np.mean([\n",
    "                             np.sum(np.square(coeffs[3]) * np.log(np.square(coeffs[3]))),\n",
    "                             np.sum(np.square(coeffs[2]) * np.log(np.square(coeffs[2]))),\n",
    "                             np.sum(np.square(coeffs[1]) * np.log(np.square(coeffs[1])))])\n",
    "\n",
    "            A_Entropy = np.sum(np.square(coeffs[0]) * np.log(np.square(coeffs[0])))\n",
    "            D_mean = np.mean([np.mean(coeffs[3]),np.mean(coeffs[2]),np.mean(coeffs[1])])\n",
    "            A_mean = np.mean(coeffs[0])\n",
    "            D_std = np.mean([np.std(coeffs[3]),np.std(coeffs[2]),np.std(coeffs[1])])\n",
    "            A_std = np.std(coeffs[0])\n",
    "            features = [cD_Energy,cA_Energy,D_Entropy,A_Entropy,D_mean,A_mean,D_std,A_std]\n",
    "        all_features.append(features)\n",
    "    return np.array(all_features)\n"
   ],
   "id": "8605e18ca885d8dc",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T17:41:50.036250Z",
     "start_time": "2025-07-21T17:41:50.023244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_X_and_Y_from_epochs_with_WT(train_list, test_list, events, picks=None, t_min = -0.2, t_max = 0.5):\n",
    "\n",
    "    epochs_train, epochs_test = read_eeg_epochs(train_list, test_list)\n",
    "\n",
    "    #####---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    epochs_train_list_event1 = epochs_train[events[0]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    epochs_train_list_event2 = epochs_train[events[1]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    X_train = np.concatenate((epochs_train_list_event1, epochs_train_list_event2), axis=0)\n",
    "\n",
    "    labels_up_train = [0] * len(epochs_train_list_event1)\n",
    "    labels_inv_train = [1] * len(epochs_train_list_event2)\n",
    "    y_train = np.concatenate((labels_up_train, labels_inv_train), axis=0)\n",
    "\n",
    "    ######--------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    epochs_test_list_event1 = epochs_test[events[0]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    epochs_test_list_event2 = epochs_test[events[1]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    X_test = np.concatenate((epochs_test_list_event1, epochs_test_list_event2), axis=0)\n",
    "\n",
    "    labels_up_test = [0] * len(epochs_test_list_event1)\n",
    "    labels_inv_test = [1] * len(epochs_test_list_event2)\n",
    "    y_test = np.concatenate((labels_up_test, labels_inv_test), axis=0)\n",
    "\n",
    "    #####---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    X_train_feats = wavelet_avg_features(X_train)\n",
    "    X_test_feats = wavelet_avg_features(X_test)\n",
    "\n",
    "    logging.info(f\"shape: {X_train_feats.shape}\")\n",
    "\n",
    "    return X_train_feats, X_test_feats, y_train, y_test"
   ],
   "id": "587b15546b9f14df",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T17:41:52.045227Z",
     "start_time": "2025-07-21T17:41:52.033219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_X_and_Y_from_epochs_with_band_power_feature_extraction(train_list, test_list, events, picks=None):\n",
    "\n",
    "    t_min = -0.2\n",
    "    t_max = 0.5\n",
    "    epochs_train, epochs_test = read_eeg_epochs(train_list, test_list)\n",
    "\n",
    "    #####---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    epochs_train_list_event1 = epochs_train[events[0]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    epochs_train_list_event2 = epochs_train[events[1]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    X_train = np.concatenate((epochs_train_list_event1, epochs_train_list_event2), axis=0)\n",
    "\n",
    "    labels_up_train = [0] * len(epochs_train_list_event1)\n",
    "    labels_inv_train = [1] * len(epochs_train_list_event2)\n",
    "    y_train = np.concatenate((labels_up_train, labels_inv_train), axis=0)\n",
    "\n",
    "    ######--------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    epochs_test_list_event1 = epochs_test[events[0]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    epochs_test_list_event2 = epochs_test[events[1]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    X_test = np.concatenate((epochs_test_list_event1, epochs_test_list_event2), axis=0)\n",
    "\n",
    "    labels_up_test = [0] * len(epochs_test_list_event1)\n",
    "    labels_inv_test = [1] * len(epochs_test_list_event2)\n",
    "    y_test = np.concatenate((labels_up_test, labels_inv_test), axis=0)\n",
    "\n",
    "    #####---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    times = np.linspace(t_min, t_max, X_train.shape[2])\n",
    "    # Ekstrakcja cech peaków\n",
    "    X_train_feats = extract_band_power_features(X_train, times)\n",
    "    X_test_feats = extract_band_power_features(X_test, times)\n",
    "\n",
    "    logging.info(f\"shape: {X_train_feats.shape}\")\n",
    "\n",
    "    return X_train_feats, X_test_feats, y_train, y_test"
   ],
   "id": "9a43ade10bc58b41",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T19:16:02.330450Z",
     "start_time": "2025-08-17T19:16:02.317451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_and_test_model(X_train, X_test, y_train, y_test, pipeline, gridSerach = False):\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # predict test data\n",
    "    y_test_pred = pipeline.predict(X_test)\n",
    "    test_score = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    # predict train data\n",
    "    y_train_pred = pipeline.predict(X_train)\n",
    "    train_score = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "    print(f\"test_score: {test_score:.4f}\")\n",
    "    print(f\"train_score: {train_score:.4f}\")\n",
    "\n",
    "    if gridSerach:\n",
    "        print(f\"The best parameters: {pipeline.best_params_}\")\n",
    "        print(f\"The best accuracy: {pipeline.best_score_:.4f}\")"
   ],
   "id": "eebac4b5b4aa7f67",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T19:15:06.099956Z",
     "start_time": "2025-08-17T19:15:06.082960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "log_file = \"training_log_3.txt\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file, mode=\"a\"), # a to overwrite\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "class StreamToLogger:\n",
    "    def __init__(self, logger, level):\n",
    "        self.logger = logger\n",
    "        self.level = level\n",
    "        self.line_buffer = \"\"\n",
    "\n",
    "    def write(self, message):\n",
    "        if message.strip():\n",
    "            self.logger.log(self.level, message.strip())\n",
    "\n",
    "    def flush(self):\n",
    "        pass\n",
    "\n",
    "sys.stdout = StreamToLogger(logging.getLogger(), logging.INFO)\n",
    "sys.stderr = StreamToLogger(logging.getLogger(), logging.ERROR)\n"
   ],
   "id": "b375c7108d1cb9af",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T19:21:35.540692Z",
     "start_time": "2025-08-17T19:21:35.525682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dir_path = 'D:\\studia\\magisterka\\dane EEG\\BADANIE_POLITYCZNE_2022_eeg_bdfy\\EEG_preprocessed'\n",
    "file_name_template = \"s*.bdf-epo.fif\"\n",
    "train_ratio = 0.8\n",
    "selected_channels = ['P5', 'P6', 'P7', 'P8','PO7', 'PO8']\n",
    "\n",
    "flatten_transformer = FunctionTransformer(lambda X: X.reshape(X.shape[0], -1))"
   ],
   "id": "84b2443e97141478",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MODEL 1: time-range 0-250 Logistic Regression",
   "id": "25dc4b449e6d6d9d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T15:47:15.934624Z",
     "start_time": "2025-07-02T15:47:15.920475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# model_1 = Pipeline(steps=[('reshape', flatten_transformer), ('scaler', StandardScaler()), ('logisticRegression', LogisticRegression(max_iter=10000))])\n",
    "#\n",
    "# train_list, test_list = split_train_test_path_list(dir_path, file_name_template, train_ratio)\n",
    "# X_train, X_test, y_train, y_test = get_X_and_Y_from_epochs(train_list, test_list, [\"up\", \"inv\"], t_min = 0.0, t_max = 0.25)\n",
    "#\n",
    "# train_and_test_model(X_train, X_test, y_train, y_test, model_1)"
   ],
   "id": "d78fbc30a8e8a399",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### MODEL 2: Support Vector Machine with grid search\n",
   "id": "6a3549f491240355"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T15:47:15.966489Z",
     "start_time": "2025-07-02T15:47:15.952474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# model_2 = Pipeline(steps=[('reshape', flatten_transformer), ('scaler', StandardScaler()), ('svc', SVC())])\n",
    "#\n",
    "# param_grid = dict(\n",
    "#     svc__kernel=['linear'],\n",
    "#     svc__C=[0.1, 1.0],\n",
    "#     svc__gamma=[0.001, 0.01],\n",
    "# )\n",
    "#\n",
    "# logging.info(\"Rozpoczynam trenowanie modelu...\")\n",
    "# train_list, test_list = split_train_test_path_list(dir_path, file_name_template, train_ratio)\n",
    "# X_train, X_test, y_train, y_test = get_X_and_Y_from_epochs(train_list, test_list, [\"up\", \"inv\"], t_min = 0.0, t_max = 0.25)\n",
    "# logging.info(\"Rozpoczęto GridSearchCV.\")\n",
    "# with parallel_backend('multiprocessing'):\n",
    "#     grid_search_model_2 = GridSearchCV(model_2, param_grid, cv=3, scoring='accuracy', n_jobs = -1, verbose=3)\n",
    "#     train_and_test_model(X_train, X_test, y_train, y_test, grid_search_model_2, True)\n",
    "#\n",
    "#\n",
    "# logging.info(f\"Najlepsze parametry: {grid_search_model_2.best_params_}\")\n",
    "# logging.info(f\"Najlepszy wynik cross-validation: {grid_search_model_2.best_score_}\")\n",
    "# logging.info(\"Trenowanie zakończone.\")"
   ],
   "id": "5c96ea5fa284b9ee",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T21:12:03.480342Z",
     "start_time": "2025-07-10T21:11:51.275241Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "72b670384db65e56",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T13:16:08.620285Z",
     "start_time": "2025-08-17T13:15:52.862634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_list, test_list = split_train_test_path_list(dir_path, file_name_template, train_ratio)\n",
    "X_train, X_test, y_train, y_test = get_X_and_Y_from_epochs(train_list, test_list, [\"in\", \"out\"])"
   ],
   "id": "d51b27c415d42792",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "18683 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "4653 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "['Fp1', 'AF7', 'AF3', 'F1', 'F3', 'F5', 'F7', 'FT7', 'FC5', 'FC3', 'FC1', 'C1', 'C3', 'C5', 'T7', 'TP7', 'CP5', 'CP3', 'CP1', 'P1', 'P3', 'P5', 'P7', 'P9', 'PO7', 'PO3', 'O1', 'Iz', 'Oz', 'POz', 'Pz', 'CPz', 'Fpz', 'Fp2', 'AF8', 'AF4', 'AFz', 'Fz', 'F2', 'F4', 'F6', 'F8', 'FT8', 'FC6', 'FC4', 'FC2', 'FCz', 'Cz', 'C2', 'C4', 'C6', 'T8', 'TP8', 'CP6', 'CP4', 'CP2', 'P2', 'P4', 'P6', 'P8', 'P10', 'PO8', 'PO4', 'O2', 'EXG1', 'EXG2', 'EXG3', 'EXG4']\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T14:09:45.336429Z",
     "start_time": "2025-08-17T14:09:45.307432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "\n",
    "def compare_events_per_epoch_peak(\n",
    "    event1, event2, sfreq, channels,\n",
    "    *,\n",
    "    tmin=0.120, tmax=0.200,      # okno analizy (N170)\n",
    "    tmin_epoch=-0.200,           # początek epoki (s)\n",
    "    peak='min',                  # 'min' (N170) lub 'max'\n",
    "    alpha=0.05,\n",
    "    fdr='bh'                     # None lub 'bh' (Benjamini–Hochberg)\n",
    "):\n",
    "    \"\"\"\n",
    "    Porównuje średnią amplitudę w oknie, amplitudę peaku i latencję peaku\n",
    "    między dwoma warunkami (event1, event2) dla każdego kanału,\n",
    "    szukając peaku **per epoka**.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    event1, event2 : ndarray [n_epochs, n_channels, n_times]\n",
    "    sfreq : float, Hz\n",
    "    channels : list[str]\n",
    "    tmin, tmax : float, granice okna (s)\n",
    "    tmin_epoch : float, początek epoki (s)\n",
    "    peak : 'min' lub 'max'\n",
    "    alpha : poziom istotności\n",
    "    fdr : None lub 'bh' dla korekcji wielokrotnych porównań\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame z kolumnami:\n",
    "        channel, mean_in/out, p_mean, peak_in/out, p_peak,\n",
    "        lat_in/lat_out (s), p_lat, oraz flagi istotności (z FDR jeśli podano).\n",
    "    \"\"\"\n",
    "    # --- sanity i dopasowanie liczby epok do t-testu sparowanego\n",
    "    X1 = np.asarray(event1); X2 = np.asarray(event2)\n",
    "    assert X1.ndim == 3 and X2.ndim == 3, \"Wejście musi mieć kształt [n_epochs, n_channels, n_times]\"\n",
    "    assert X1.shape[1:] == X2.shape[1:], \"Musi być ta sama liczba kanałów i próbek czasu\"\n",
    "    n1, n_channels, n_times = X1.shape\n",
    "    n2 = X2.shape[0]\n",
    "    n = min(n1, n2)\n",
    "    if n1 != n2:\n",
    "        X1 = X1[:n]; X2 = X2[:n]\n",
    "\n",
    "    if channels is None or len(channels) != n_channels:\n",
    "        channels = [f\"ch{idx}\" for idx in range(n_channels)]\n",
    "\n",
    "    # --- wektor czasu i okno\n",
    "    times = np.arange(n_times) / float(sfreq) + float(tmin_epoch)\n",
    "    idx_tmin = int(np.argmin(np.abs(times - tmin)))\n",
    "    idx_tmax = int(np.argmin(np.abs(times - tmax)))\n",
    "    if idx_tmax <= idx_tmin:\n",
    "        raise ValueError(\"tmax musi być > tmin (sekundy).\")\n",
    "\n",
    "    if peak == 'max':\n",
    "        peak_fun = np.argmax\n",
    "    elif peak == 'min':\n",
    "        peak_fun = np.argmin\n",
    "    else:\n",
    "        raise ValueError(\"peak ∈ {'min','max'}\")\n",
    "\n",
    "    rows = []\n",
    "    for ch_idx, ch in enumerate(channels):\n",
    "        d1 = X1[:, ch_idx, idx_tmin:idx_tmax]  # [n, n_tw]\n",
    "        d2 = X2[:, ch_idx, idx_tmin:idx_tmax]\n",
    "\n",
    "        # średnia amplituda w oknie (per epoka)\n",
    "        mean1 = d1.mean(axis=1)\n",
    "        mean2 = d2.mean(axis=1)\n",
    "        _, p_mean = ttest_rel(mean1, mean2, nan_policy='omit')\n",
    "\n",
    "        # --- peak i latencja **per epoka**\n",
    "        peak_idx1 = peak_fun(d1, axis=1)                 # [n]\n",
    "        peak_idx2 = peak_fun(d2, axis=1)                 # [n]\n",
    "        amp1 = d1[np.arange(n), peak_idx1]               # amplituda w peaku, per epoka\n",
    "        amp2 = d2[np.arange(n), peak_idx2]\n",
    "        # latencja w sekundach: start okna + indeks/sfreq\n",
    "        lat1 = times[idx_tmin] + peak_idx1 / float(sfreq)\n",
    "        lat2 = times[idx_tmin] + peak_idx2 / float(sfreq)\n",
    "\n",
    "        # testy\n",
    "        _, p_peak = ttest_rel(amp1, amp2, nan_policy='omit')\n",
    "        _, p_lat  = ttest_rel(lat1,  lat2,  nan_policy='omit')\n",
    "\n",
    "        rows.append({\n",
    "            \"channel\": ch,\n",
    "            \"mean_in\": float(np.nanmean(mean1)),\n",
    "            \"mean_out\": float(np.nanmean(mean2)),\n",
    "            \"p_mean\": float(p_mean),\n",
    "\n",
    "            \"peak_in\": float(np.nanmean(amp1)),\n",
    "            \"peak_out\": float(np.nanmean(amp2)),\n",
    "            \"p_peak\": float(p_peak),\n",
    "\n",
    "            \"lat_in\": float(np.nanmean(lat1)),\n",
    "            \"lat_out\": float(np.nanmean(lat2)),\n",
    "            \"p_lat\": float(p_lat),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df"
   ],
   "id": "12cd4421fe3c14e1",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T19:04:30.616402Z",
     "start_time": "2025-08-17T19:04:29.813341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "event1, event2 = read_all_epochs(dir_path, file_name_template, [\"in\", \"out\"], selected_channels)\n",
    "\n",
    "results = compare_events_per_epoch_peak(event1, event2, 128, selected_channels)\n",
    "\n",
    "print(results)"
   ],
   "id": "e6f9b5151e3e149",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dir_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m selected_channels \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mP3\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mP4\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mP5\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mP6\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mP7\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mP8\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPO3\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPO4\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPO7\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPO8\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m----> 2\u001B[0m event1, event2 \u001B[38;5;241m=\u001B[39m read_all_epochs(\u001B[43mdir_path\u001B[49m, file_name_template, [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124min\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mout\u001B[39m\u001B[38;5;124m\"\u001B[39m], selected_channels)\n\u001B[0;32m      4\u001B[0m results \u001B[38;5;241m=\u001B[39m compare_events_per_epoch_peak(event1, event2, \u001B[38;5;241m128\u001B[39m, selected_channels)\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(results)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'dir_path' is not defined"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T14:15:49.814685Z",
     "start_time": "2025-08-17T14:15:49.800336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def summarize_significant_channels(df, alpha=0.05, use_fdr=True,\n",
    "                                   metrics=(\"mean\", \"peak\", \"lat\")):\n",
    "    \"\"\"\n",
    "    Zwraca słownik i czytelny tekst z listą kanałów,\n",
    "    na których zaobserwowano istotne różnice dla wybranych metryk.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Wynik compare_events_per_epoch_peak(...) lub Twojej compare_events(...).\n",
    "    alpha : float\n",
    "        Poziom istotności.\n",
    "    use_fdr : bool\n",
    "        Jeśli True, używa kolumn p_*_fdr, jeśli istnieją; w przeciwnym razie p_*.\n",
    "    metrics : tuple[str]\n",
    "        Podzbiór z {\"mean\",\"peak\",\"lat\"}.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    summary_dict : dict\n",
    "        Np. {\"mean\": [\"P3\",\"PO7\"], \"peak\": [\"P4\"], \"lat\": []}\n",
    "    summary_text : str\n",
    "        Zwięzły opis do wydruku.\n",
    "    \"\"\"\n",
    "    summary = {}\n",
    "    lines = []\n",
    "\n",
    "    for m in metrics:\n",
    "        # wybór właściwej kolumny p\n",
    "        p_col_fdr = f\"p_{m}_fdr\"\n",
    "        p_col_raw = f\"p_{m}\"\n",
    "        if use_fdr and p_col_fdr in df.columns:\n",
    "            p_col = p_col_fdr\n",
    "            note = \"po FDR\"\n",
    "        else:\n",
    "            p_col = p_col_raw\n",
    "            note = \"bez korekcji\"\n",
    "\n",
    "        if p_col not in df.columns:\n",
    "            summary[m] = []\n",
    "            lines.append(f\"- {m}: brak kolumny {p_col} w df.\")\n",
    "            continue\n",
    "\n",
    "        sig_mask = df[p_col] < alpha\n",
    "        chans = df.loc[sig_mask, \"channel\"].tolist()\n",
    "        summary[m] = chans\n",
    "\n",
    "        if chans:\n",
    "            lines.append(f\"- {m} ({note}, α={alpha}): {', '.join(chans)}\")\n",
    "        else:\n",
    "            lines.append(f\"- {m} ({note}, α={alpha}): brak istotnych kanałów\")\n",
    "\n",
    "    # kanały istotne w którejkolwiek metryce (uniona)\n",
    "    any_sig = sorted(set().union(*summary.values())) if summary else []\n",
    "    if any_sig:\n",
    "        lines.append(f\"\\nKanały istotne w ≥1 metryce: {', '.join(any_sig)}\")\n",
    "    else:\n",
    "        lines.append(\"\\nBrak kanałów istotnych w jakiejkolwiek metryce.\")\n",
    "\n",
    "    return summary, \"\\n\".join(lines)"
   ],
   "id": "332b8113ec178589",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T14:15:55.572187Z",
     "start_time": "2025-08-17T14:15:55.554857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "summary, text = summarize_significant_channels(results, alpha=0.05, use_fdr=True)\n",
    "print(text)"
   ],
   "id": "4257e4e77e1fdd45",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- mean (bez korekcji, α=0.05): brak istotnych kanałów\n",
      "- peak (bez korekcji, α=0.05): brak istotnych kanałów\n",
      "- lat (bez korekcji, α=0.05): P5, PO8\n",
      "\n",
      "Kanały istotne w ≥1 metryce: P5, PO8\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "'Fp1', 'AF7', 'AF3', 'F1', 'F3', 'F5', 'F7', 'FT7', 'FC5', 'FC3', 'FC1', 'C1', 'C3', 'C5', 'T7', 'TP7', 'CP5', 'CP3', 'CP1', 'P1', 'P3', 'P5', 'P7', 'P9', 'PO7', 'PO3', 'O1', 'Iz', 'Oz', 'POz', 'Pz', 'CPz', 'Fpz', 'Fp2', 'AF8', 'AF4', 'AFz', 'Fz', 'F2', 'F4', 'F6', 'F8', 'FT8', 'FC6', 'FC4', 'FC2', 'FCz', 'Cz', 'C2', 'C4', 'C6', 'T8', 'TP8', 'CP6', 'CP4', 'CP2', 'P2', 'P4', 'P6', 'P8', 'P10', 'PO8', 'PO4', 'O2', 'EXG1', 'EXG2', 'EXG3', 'EXG4'",
   "id": "d3fedd6bb1070cb6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model 3: LinearDiscriminantAnalysis",
   "id": "c5570c71213c3ff3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T00:40:22.989876Z",
     "start_time": "2025-07-20T00:40:21.101447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model_lda = Pipeline(steps=[\n",
    "    ('reshape', flatten_transformer),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lda', LinearDiscriminantAnalysis())\n",
    "])\n",
    "\n",
    "logging.info(\"Rozpoczynam trenowanie modelu LDA...\")\n",
    "train_list, test_list = split_train_test_path_list(dir_path, file_name_template, train_ratio)\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_X_and_Y_from_epochs_with_feature_extraction(train_list, test_list, [\"in\", \"out\"], selected_channels, window_size=0.03, baseline_correction = True)\n",
    "\n",
    "train_and_test_model(X_train, X_test, y_train, y_test, model_lda)\n",
    "\n",
    "logging.info(\"Trenowanie zakończone.\")\n"
   ],
   "id": "95ec7ade5d5c7ed5",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[21], line 15\u001B[0m\n\u001B[0;32m     12\u001B[0m logging\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRozpoczynam trenowanie modelu LDA...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     13\u001B[0m train_list, test_list \u001B[38;5;241m=\u001B[39m split_train_test_path_list(dir_path, file_name_template, train_ratio)\n\u001B[1;32m---> 15\u001B[0m X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m \u001B[43mget_X_and_Y_from_epochs_with_feature_extraction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43min\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mout\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mselected_channels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwindow_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.03\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbaseline_correction\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m train_and_test_model(X_train, X_test, y_train, y_test, model_lda)\n\u001B[0;32m     19\u001B[0m logging\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrenowanie zakończone.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[11], line 5\u001B[0m, in \u001B[0;36mget_X_and_Y_from_epochs_with_feature_extraction\u001B[1;34m(train_list, test_list, events, picks, window_size, baseline_correction)\u001B[0m\n\u001B[0;32m      3\u001B[0m t_min \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m0.2\u001B[39m\n\u001B[0;32m      4\u001B[0m t_max \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.5\u001B[39m\n\u001B[1;32m----> 5\u001B[0m epochs_train, epochs_test \u001B[38;5;241m=\u001B[39m \u001B[43mread_eeg_epochs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_list\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m#####---------------------------------------------------------------------------------------------------------\u001B[39;00m\n\u001B[0;32m      9\u001B[0m epochs_train_list_event1 \u001B[38;5;241m=\u001B[39m epochs_train[events[\u001B[38;5;241m0\u001B[39m]]\u001B[38;5;241m.\u001B[39mget_data(picks\u001B[38;5;241m=\u001B[39mpicks, tmin\u001B[38;5;241m=\u001B[39mt_min, tmax\u001B[38;5;241m=\u001B[39mt_max)\n",
      "Cell \u001B[1;32mIn[7], line 7\u001B[0m, in \u001B[0;36mread_eeg_epochs\u001B[1;34m(train_list, test_list)\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m file_path \u001B[38;5;129;01min\u001B[39;00m train_list:\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m mne\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39muse_log_level(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mERROR\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m----> 7\u001B[0m         epoch_train \u001B[38;5;241m=\u001B[39m \u001B[43mmne\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_epochs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreload\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m         epochs_train_list\u001B[38;5;241m.\u001B[39mappend(epoch_train)\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m file_path \u001B[38;5;129;01min\u001B[39;00m test_list:\n",
      "File \u001B[1;32m<decorator-gen-232>:12\u001B[0m, in \u001B[0;36mread_epochs\u001B[1;34m(fname, proj, preload, verbose)\u001B[0m\n",
      "File \u001B[1;32m~\\PycharmProjects\\PythonProject\\.venv\\lib\\site-packages\\mne\\epochs.py:4231\u001B[0m, in \u001B[0;36mread_epochs\u001B[1;34m(fname, proj, preload, verbose)\u001B[0m\n\u001B[0;32m   4213\u001B[0m \u001B[38;5;129m@verbose\u001B[39m\n\u001B[0;32m   4214\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mread_epochs\u001B[39m(fname, proj\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, preload\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpochsFIF\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m   4215\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Read epochs from a fif file.\u001B[39;00m\n\u001B[0;32m   4216\u001B[0m \n\u001B[0;32m   4217\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4229\u001B[0m \u001B[38;5;124;03m        The epochs.\u001B[39;00m\n\u001B[0;32m   4230\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 4231\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mEpochsFIF\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreload\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m<decorator-gen-233>:12\u001B[0m, in \u001B[0;36m__init__\u001B[1;34m(self, fname, proj, preload, verbose)\u001B[0m\n",
      "File \u001B[1;32m~\\PycharmProjects\\PythonProject\\.venv\\lib\\site-packages\\mne\\epochs.py:4398\u001B[0m, in \u001B[0;36mEpochsFIF.__init__\u001B[1;34m(self, fname, proj, preload, verbose)\u001B[0m\n\u001B[0;32m   4393\u001B[0m drop_log \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(drop_log[:step])\n\u001B[0;32m   4395\u001B[0m \u001B[38;5;66;03m# call BaseEpochs constructor\u001B[39;00m\n\u001B[0;32m   4396\u001B[0m \u001B[38;5;66;03m# again, ensure we're retaining the baseline period originally loaded\u001B[39;00m\n\u001B[0;32m   4397\u001B[0m \u001B[38;5;66;03m# from disk without trying to re-apply baseline correction\u001B[39;00m\n\u001B[1;32m-> 4398\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\n\u001B[0;32m   4399\u001B[0m     info,\n\u001B[0;32m   4400\u001B[0m     data,\n\u001B[0;32m   4401\u001B[0m     events,\n\u001B[0;32m   4402\u001B[0m     event_id,\n\u001B[0;32m   4403\u001B[0m     tmin,\n\u001B[0;32m   4404\u001B[0m     tmax,\n\u001B[0;32m   4405\u001B[0m     baseline\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   4406\u001B[0m     raw\u001B[38;5;241m=\u001B[39mraw,\n\u001B[0;32m   4407\u001B[0m     proj\u001B[38;5;241m=\u001B[39mproj,\n\u001B[0;32m   4408\u001B[0m     preload_at_end\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m   4409\u001B[0m     on_missing\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   4410\u001B[0m     selection\u001B[38;5;241m=\u001B[39mselection,\n\u001B[0;32m   4411\u001B[0m     drop_log\u001B[38;5;241m=\u001B[39mdrop_log,\n\u001B[0;32m   4412\u001B[0m     filename\u001B[38;5;241m=\u001B[39mfname_rep,\n\u001B[0;32m   4413\u001B[0m     metadata\u001B[38;5;241m=\u001B[39mmetadata,\n\u001B[0;32m   4414\u001B[0m     verbose\u001B[38;5;241m=\u001B[39mverbose,\n\u001B[0;32m   4415\u001B[0m     raw_sfreq\u001B[38;5;241m=\u001B[39mraw_sfreq,\n\u001B[0;32m   4416\u001B[0m     annotations\u001B[38;5;241m=\u001B[39mannotations,\n\u001B[0;32m   4417\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mreject_params,\n\u001B[0;32m   4418\u001B[0m )\n\u001B[0;32m   4419\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbaseline \u001B[38;5;241m=\u001B[39m baseline\n\u001B[0;32m   4420\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_baseline \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m<decorator-gen-212>:12\u001B[0m, in \u001B[0;36m__init__\u001B[1;34m(self, info, data, events, event_id, tmin, tmax, baseline, raw, picks, reject, flat, decim, reject_tmin, reject_tmax, detrend, proj, on_missing, preload_at_end, selection, drop_log, filename, metadata, event_repeated, raw_sfreq, annotations, verbose)\u001B[0m\n",
      "File \u001B[1;32m~\\PycharmProjects\\PythonProject\\.venv\\lib\\site-packages\\mne\\epochs.py:577\u001B[0m, in \u001B[0;36mBaseEpochs.__init__\u001B[1;34m(***failed resolving arguments***)\u001B[0m\n\u001B[0;32m    574\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdetrend \u001B[38;5;241m=\u001B[39m detrend\n\u001B[0;32m    576\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_raw \u001B[38;5;241m=\u001B[39m raw\n\u001B[1;32m--> 577\u001B[0m \u001B[43minfo\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_consistency\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    578\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpicks \u001B[38;5;241m=\u001B[39m _picks_to_idx(\n\u001B[0;32m    579\u001B[0m     info, picks, none\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m\"\u001B[39m, exclude\u001B[38;5;241m=\u001B[39m(), allow_empty\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    580\u001B[0m )\n\u001B[0;32m    581\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo \u001B[38;5;241m=\u001B[39m pick_info(info, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpicks)\n",
      "File \u001B[1;32m~\\PycharmProjects\\PythonProject\\.venv\\lib\\site-packages\\mne\\_fiff\\meas_info.py:1913\u001B[0m, in \u001B[0;36mInfo._check_consistency\u001B[1;34m(self, prepend_error)\u001B[0m\n\u001B[0;32m   1911\u001B[0m \u001B[38;5;66;03m# make sure channel names are unique\u001B[39;00m\n\u001B[0;32m   1912\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_unlock():\n\u001B[1;32m-> 1913\u001B[0m     \u001B[38;5;28mself\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mch_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43m_unique_channel_names\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mch_names\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1914\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m idx, ch_name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mch_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]):\n\u001B[0;32m   1915\u001B[0m         \u001B[38;5;28mself\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mchs\u001B[39m\u001B[38;5;124m\"\u001B[39m][idx][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mch_name\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m ch_name\n",
      "File \u001B[1;32m<decorator-gen-21>:12\u001B[0m, in \u001B[0;36m_unique_channel_names\u001B[1;34m(ch_names, max_length, verbose)\u001B[0m\n",
      "File \u001B[1;32m~\\PycharmProjects\\PythonProject\\.venv\\lib\\site-packages\\mne\\_fiff\\meas_info.py:276\u001B[0m, in \u001B[0;36m_unique_channel_names\u001B[1;34m(ch_names, max_length, verbose)\u001B[0m\n\u001B[0;32m    274\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m max_length \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    275\u001B[0m     ch_names[:] \u001B[38;5;241m=\u001B[39m [name[:max_length] \u001B[38;5;28;01mfor\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m ch_names]\n\u001B[1;32m--> 276\u001B[0m unique_ids \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munique\u001B[49m\u001B[43m(\u001B[49m\u001B[43mch_names\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m    277\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(unique_ids) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(ch_names):\n\u001B[0;32m    278\u001B[0m     dups \u001B[38;5;241m=\u001B[39m {ch_names[x] \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m np\u001B[38;5;241m.\u001B[39msetdiff1d(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(ch_names)), unique_ids)}\n",
      "File \u001B[1;32m~\\PycharmProjects\\PythonProject\\.venv\\lib\\site-packages\\numpy\\lib\\_arraysetops_impl.py:286\u001B[0m, in \u001B[0;36munique\u001B[1;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001B[0m\n\u001B[0;32m    284\u001B[0m ar \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masanyarray(ar)\n\u001B[0;32m    285\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m axis \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 286\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43m_unique1d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mar\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_inverse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_counts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    287\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mequal_nan\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mequal_nan\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minverse_shape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mar\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    288\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _unpack_tuple(ret)\n\u001B[0;32m    290\u001B[0m \u001B[38;5;66;03m# axis was specified and not None\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\PythonProject\\.venv\\lib\\site-packages\\numpy\\lib\\_arraysetops_impl.py:350\u001B[0m, in \u001B[0;36m_unique1d\u001B[1;34m(ar, return_index, return_inverse, return_counts, equal_nan, inverse_shape, axis)\u001B[0m\n\u001B[0;32m    347\u001B[0m optional_indices \u001B[38;5;241m=\u001B[39m return_index \u001B[38;5;129;01mor\u001B[39;00m return_inverse\n\u001B[0;32m    349\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m optional_indices:\n\u001B[1;32m--> 350\u001B[0m     perm \u001B[38;5;241m=\u001B[39m \u001B[43mar\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margsort\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkind\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmergesort\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mreturn_index\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mquicksort\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    351\u001B[0m     aux \u001B[38;5;241m=\u001B[39m ar[perm]\n\u001B[0;32m    352\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model 4 Gradient Boosting Classifier\n",
   "id": "c040cfeaadfdd036"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T20:10:00.455580Z",
     "start_time": "2025-08-17T19:57:01.890691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model_gbc = Pipeline(steps=[\n",
    "    ('reshape', flatten_transformer),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('gbc', GradientBoostingClassifier(n_estimators=200))\n",
    "])\n",
    "\n",
    "logging.info(\"Rozpoczynam trenowanie modelu GBC...\")\n",
    "train_list, test_list = split_train_test_path_list(dir_path, file_name_template, train_ratio)\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_X_and_Y_from_epochs_with_feature_extraction(train_list, test_list, [\"inv\", \"up\"], selected_channels)\n",
    "\n",
    "train_and_test_model(X_train, X_test, y_train, y_test, model_gbc)\n",
    "\n",
    "logging.info(\"Trenowanie zakończone.\")"
   ],
   "id": "72480449ea68e74a",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T19:16:11.369696Z",
     "start_time": "2025-08-17T19:16:11.328562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_gbc = Pipeline(steps=[\n",
    "    ('reshape', flatten_transformer),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('gbc', GradientBoostingClassifier(n_estimators=200))\n",
    "])\n",
    "\n",
    "logging.info(\"Rozpoczynam trenowanie modelu GBC...\")\n",
    "train_list, test_list = split_train_test_path_list(dir_path, file_name_template, train_ratio)\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_X_and_Y_from_epochs(train_list, test_list, [\"inv/in\", \"inv/out\"], selected_channels)\n",
    "\n",
    "train_and_test_model(X_train, X_test, y_train, y_test, model_gbc)\n",
    "\n",
    "logging.info(\"Trenowanie zakończone.\")"
   ],
   "id": "825c2f4dd0673c76",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_X_and_Y_from_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 10\u001B[0m\n\u001B[0;32m      7\u001B[0m logging\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRozpoczynam trenowanie modelu GBC...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      8\u001B[0m train_list, test_list \u001B[38;5;241m=\u001B[39m split_train_test_path_list(dir_path, file_name_template, train_ratio)\n\u001B[1;32m---> 10\u001B[0m X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m \u001B[43mget_X_and_Y_from_epochs\u001B[49m(train_list, test_list, [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minv/in\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minv/out\u001B[39m\u001B[38;5;124m\"\u001B[39m], selected_channels)\n\u001B[0;32m     12\u001B[0m train_and_test_model(X_train, X_test, y_train, y_test, model_gbc)\n\u001B[0;32m     14\u001B[0m logging\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrenowanie zakończone.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'get_X_and_Y_from_epochs' is not defined"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T19:16:21.959922Z",
     "start_time": "2025-08-17T19:16:21.913935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model_gbc = Pipeline(steps=[\n",
    "    ('reshape', flatten_transformer),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('gbc', GradientBoostingClassifier(n_estimators=200))\n",
    "])\n",
    "\n",
    "logging.info(\"Rozpoczynam trenowanie modelu GBC...\")\n",
    "train_list, test_list = split_train_test_path_list(dir_path, file_name_template, train_ratio)\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_X_and_Y_from_epochs_with_WT(train_list, test_list, [\"in\", \"out\"],\n",
    "                                                                   selected_channels)\n",
    "\n",
    "train_and_test_model(X_train, X_test, y_train, y_test, model_gbc)\n",
    "\n",
    "logging.info(\"Trenowanie zakończone.\")"
   ],
   "id": "7a0db7f83e2692ac",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_X_and_Y_from_epochs_with_WT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 14\u001B[0m\n\u001B[0;32m     11\u001B[0m logging\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRozpoczynam trenowanie modelu GBC...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     12\u001B[0m train_list, test_list \u001B[38;5;241m=\u001B[39m split_train_test_path_list(dir_path, file_name_template, train_ratio)\n\u001B[1;32m---> 14\u001B[0m X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m \u001B[43mget_X_and_Y_from_epochs_with_WT\u001B[49m(train_list, test_list, [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124min\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mout\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m     15\u001B[0m                                                                    selected_channels)\n\u001B[0;32m     17\u001B[0m train_and_test_model(X_train, X_test, y_train, y_test, model_gbc)\n\u001B[0;32m     19\u001B[0m logging\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrenowanie zakończone.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'get_X_and_Y_from_epochs_with_WT' is not defined"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T02:42:10.566956Z",
     "start_time": "2025-07-08T22:45:18.329396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model_gbc = Pipeline(steps=[\n",
    "    ('reshape', flatten_transformer),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('gbc', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'gbc__n_estimators': [100, 200],\n",
    "    'gbc__learning_rate': [0.1, 0.05, 0.01],\n",
    "    'gbc__max_depth': [3, 4, 5],\n",
    "    'gbc__subsample': [1.0, 0.8],\n",
    "    'gbc__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "train_list, test_list = split_train_test_path_list(dir_path, file_name_template, train_ratio)\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_X_and_Y_from_epochs_with_feature_extraction(train_list, test_list, [\"in\", \"out\"],  selected_channels, window_size=0.03)\n",
    "\n",
    "grid_search = GridSearchCV(model_gbc, param_grid, cv=3, n_jobs=4, verbose=2)\n",
    "\n",
    "logging.info(\"Rozpoczynam trenowanie modelu GBC z GridSearch...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "logging.info(\"Trenowanie zakończone.\")\n",
    "logging.info(f\"Najlepsze parametry: {grid_search.best_params_}\")\n",
    "logging.info(f\"Najlepszy wynik cross-val: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Testowanie na test secie\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "logging.info(f\"Dokładność na zbiorze testowym: {test_score:.4f}\")"
   ],
   "id": "f043a7c4bdf12a75",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T10:57:53.622497Z",
     "start_time": "2025-07-05T10:57:39.230129Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "18695 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "4641 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "(18695, 2, 90)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "extract_N170_peak_features2() got an unexpected keyword argument 'window_size'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 93\u001B[0m\n\u001B[0;32m     89\u001B[0m     X_train_feats \u001B[38;5;241m=\u001B[39m extract_N170_peak_features2(X_train, times, window_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.02\u001B[39m)\n\u001B[0;32m     92\u001B[0m train_list, test_list \u001B[38;5;241m=\u001B[39m split_train_test_path_list(dir_path, file_name_template, train_ratio)\n\u001B[1;32m---> 93\u001B[0m \u001B[43mget_X_and_Y_from_epochs2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43min\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mout\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mselected_channels\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[16], line 89\u001B[0m, in \u001B[0;36mget_X_and_Y_from_epochs2\u001B[1;34m(train_list, test_list, events, picks, t_min, t_max)\u001B[0m\n\u001B[0;32m     87\u001B[0m times \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mlinspace(t_min, t_max, X_train\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m])\n\u001B[0;32m     88\u001B[0m \u001B[38;5;28mprint\u001B[39m(X_train\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m---> 89\u001B[0m X_train_feats \u001B[38;5;241m=\u001B[39m \u001B[43mextract_N170_peak_features2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwindow_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.02\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: extract_N170_peak_features2() got an unexpected keyword argument 'window_size'"
     ]
    }
   ],
   "execution_count": 16,
   "source": [
    "#### TESTING to find the best window to looking for peaks\n",
    "\n",
    "dir_path = 'D:\\studia\\magisterka\\dane EEG\\BADANIE_POLITYCZNE_2022_eeg_bdfy\\EEG_preprocessed'\n",
    "file_name_template = \"s*.bdf-epo.fif\"\n",
    "train_ratio = 0.8\n",
    "selected_channels = ['P7', 'P8']\n",
    "\n",
    "def extract_N170_peak_features2(X, times):\n",
    "\n",
    "    peak_window=(0.12, 0.22)\n",
    "    n_epochs, n_channels, n_times = X.shape\n",
    "    win_mask = (times >= peak_window[0]) & (times <= peak_window[1])\n",
    "    summary = []\n",
    "\n",
    "    for ch in range(n_channels):\n",
    "        count_good = 0\n",
    "        for ep in range(n_epochs):\n",
    "            signal = X[ep, ch, :]\n",
    "            sig_win = signal[win_mask]\n",
    "\n",
    "            peaks_min, _ = find_peaks(-sig_win, distance=100) ## distance is set to 100, to find exactly one peak (because number of timepoints in my epoch is 90)\n",
    "            n_min = len(peaks_min)\n",
    "            if n_min == 1:\n",
    "                count_good += 1\n",
    "        percent = 100 * count_good / n_epochs\n",
    "        print(f\"Channel {ch} ({ch if hasattr(X, 'ch_names') else ''}): {percent:.1f}% epoch with n170 peak\")\n",
    "        summary.append(percent)\n",
    "\n",
    "    return summary\n",
    "\n",
    "def extract_P100_peak_features2(X, times):\n",
    "\n",
    "    peak_window=(0.05, 0.15)\n",
    "    n_epochs, n_channels, n_times = X.shape\n",
    "    win_mask = (times >= peak_window[0]) & (times <= peak_window[1])\n",
    "    summary = []\n",
    "\n",
    "    for ch in range(n_channels):\n",
    "        count_good = 0\n",
    "        for ep in range(n_epochs):\n",
    "            signal = X[ep, ch, :]\n",
    "            sig_win = signal[win_mask]\n",
    "\n",
    "            peaks_max, _ = find_peaks(sig_win, distance=100) ## distance is set to 100, to find exactly one peak (because number of timepoints in my epoch is 90)\n",
    "            n_max = len(peaks_max)\n",
    "            if n_max == 1:\n",
    "                count_good += 1\n",
    "        percent = 100 * count_good / n_epochs\n",
    "        print(f\"Channel {ch} ({ch if hasattr(X, 'ch_names') else ''}): {percent:.1f}% epoch with p100 peak\")\n",
    "        summary.append(percent)\n",
    "\n",
    "    return summary\n",
    "\n",
    "def extract_P300_peak(X, times):\n",
    "\n",
    "    peak_window=(0.2, 0.3)\n",
    "    n_epochs, n_channels, n_times = X.shape\n",
    "    win_mask = (times >= peak_window[0]) & (times <= peak_window[1])\n",
    "    summary = []\n",
    "\n",
    "    for ch in range(n_channels):\n",
    "        count_good = 0\n",
    "        for ep in range(n_epochs):\n",
    "            signal = X[ep, ch, :]\n",
    "            sig_win = signal[win_mask]\n",
    "            peaks_max, _ = find_peaks(sig_win, distance=100) ## distance is set to 100, to find exactly one peak (because number of timepoints in my epoch is 90)\n",
    "            n_max = len(peaks_max)\n",
    "\n",
    "            if n_max == 1:\n",
    "                count_good += 1\n",
    "        percent = 100 * count_good / n_epochs\n",
    "        print(f\"Channel {ch} ({ch if hasattr(X, 'ch_names') else ''}): {percent:.1f}% epoch with p300 peak\")\n",
    "        summary.append(percent)\n",
    "\n",
    "    return summary\n",
    "\n",
    "def get_X_and_Y_from_epochs2(train_list, test_list, events, picks=None, t_min = -0.2, t_max = 0.5):\n",
    "\n",
    "    epochs, epochs_test = read_eeg_epochs(train_list, test_list)\n",
    "\n",
    "    #####---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    epochs_list_event1 = epochs[events[0]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    epochs_list_event2 = epochs[events[1]].get_data(picks=picks, tmin=t_min, tmax=t_max)\n",
    "    X_train = np.concatenate((epochs_list_event1, epochs_list_event2), axis=0)\n",
    "\n",
    "    times = np.linspace(t_min, t_max, X_train.shape[2])\n",
    "    print(X_train.shape)\n",
    "    X_train_feats = extract_N170_peak_features2(X_train, times, window_size=0.02)\n",
    "\n",
    "\n",
    "train_list, test_list = split_train_test_path_list(dir_path, file_name_template, train_ratio)\n",
    "get_X_and_Y_from_epochs2(train_list, test_list, [\"in\", \"out\"], selected_channels)\n",
    "\n",
    "\n"
   ],
   "id": "7d21abf64e69f35b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
